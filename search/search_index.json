{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Learning Elastic Stack a.k.a ELK Stack Hi, I am Gaurav Sood, working as a DevOps Engineer I frequently use a lot of tools and technologies as part of my work. One of the technologies I work on daily basis and is greatly impressed by is the Elasticsearch Logstash and Kibana Stack also known as ELK Stack. To be honest I am just an average person learning stuff everyday and want to learn them in a manner that lasts longs and can be easily shared. I am starting this website as a part of learning the Elasticsearch which will help in getting the basics clear and have real life examples of the various use cases. So starting with this thought in mind lets get started into what is Elasticsearch. I have kept my to my best efforts to stick as close to the official Elasticsearch documentation in case of any issues you see here you can refer to the docs at https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html","title":"Welcome"},{"location":"#welcome-to-learning-elastic-stack-aka-elk-stack","text":"Hi, I am Gaurav Sood, working as a DevOps Engineer I frequently use a lot of tools and technologies as part of my work. One of the technologies I work on daily basis and is greatly impressed by is the Elasticsearch Logstash and Kibana Stack also known as ELK Stack. To be honest I am just an average person learning stuff everyday and want to learn them in a manner that lasts longs and can be easily shared. I am starting this website as a part of learning the Elasticsearch which will help in getting the basics clear and have real life examples of the various use cases. So starting with this thought in mind lets get started into what is Elasticsearch. I have kept my to my best efforts to stick as close to the official Elasticsearch documentation in case of any issues you see here you can refer to the docs at https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html","title":"Welcome to Learning Elastic Stack a.k.a ELK Stack"},{"location":"debugging/","text":"","title":"Debugging"},{"location":"Installation/install/","text":"On CentOS 7 Machine, elastic version 6.2.4 as this installs xpack, in newer versions xpack is installed by default Install java yum install java-1.8.0-openjdk -y Going via tar Start with adding a user elastic, sudo useradd elastic Increase security limits for the elastic user In /etc/security/limits.conf add elastic - nofile 65536 Increase Memory Map limits In /etc/sysctl.conf add vm.max_map_count = 262144 do sysctl -p to load system settings Download elasticsearch curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gz tar -xzvf test - start elastic and check if its working ./bin/elasticsearch -d -p pid less logs/elasticsearch kill `cat pid` Download Kibana curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gz Install x-pack curl -L -O https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.2.4.zip For Offline installation ./elasticsearch/bin/elasticsearch-plugin install file:///home/elastic/x-pack-6.2.4.zip Securing and Encryption In master node ~/elasticsearch/config/certs ~/elasticsearch/bin/x-pack/certutil ca passowod=a ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name master --dns gsood1c.mylabserver.com --ip 172.31.22.182 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data1 --dns gsood2c.mylabserver.com --ip 172.31.31.49 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data2 --dns gsood3c.mylabserver.com --ip 172.31.22.21 In other nodes copy scp data1.p12 elastic@172.31.31.49:/home/elastic/ ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: full xpack.security.transport.ssl.keystore.path: certs/master.p12 xpack.security.transport.ssl.truststore.path: certs/master.p12 xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: certs/master.p12 xpack.security.http.ssl.truststore.path: certs/master.p12 ## same in other nodes with correct keystore In Kibana ~/kibana/bin/kibana-plugin install file:///home/elastic/x-pack-6.2.4.zip","title":"Install"},{"location":"Installation/install/#going-via-tar","text":"Start with adding a user elastic, sudo useradd elastic Increase security limits for the elastic user In /etc/security/limits.conf add elastic - nofile 65536 Increase Memory Map limits In /etc/sysctl.conf add vm.max_map_count = 262144 do sysctl -p to load system settings Download elasticsearch curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gz tar -xzvf test - start elastic and check if its working ./bin/elasticsearch -d -p pid less logs/elasticsearch kill `cat pid` Download Kibana curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gz Install x-pack curl -L -O https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.2.4.zip For Offline installation ./elasticsearch/bin/elasticsearch-plugin install file:///home/elastic/x-pack-6.2.4.zip Securing and Encryption In master node ~/elasticsearch/config/certs ~/elasticsearch/bin/x-pack/certutil ca passowod=a ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name master --dns gsood1c.mylabserver.com --ip 172.31.22.182 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data1 --dns gsood2c.mylabserver.com --ip 172.31.31.49 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data2 --dns gsood3c.mylabserver.com --ip 172.31.22.21 In other nodes copy scp data1.p12 elastic@172.31.31.49:/home/elastic/ ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: full xpack.security.transport.ssl.keystore.path: certs/master.p12 xpack.security.transport.ssl.truststore.path: certs/master.p12 xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: certs/master.p12 xpack.security.http.ssl.truststore.path: certs/master.p12 ## same in other nodes with correct keystore In Kibana ~/kibana/bin/kibana-plugin install file:///home/elastic/x-pack-6.2.4.zip","title":"Going via tar"},{"location":"Logstash/Readme/","text":"Configuring Logstash on an ELK Stack Pre Requisites Admin access to ELK Stack Logstash installed [refer to helm charts for chart] X-Pack enabled Current Configuration in Ops User - logstash-ingest Roles - Superadmin Steps followed Create the following roles to be used by logstash user - logstash-ingest logstash_writer, logstash_reader POST _xpack/security/role/logstash_writer { \"cluster\": [\"manage_index_templates\", \"monitor\"], \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"write\",\"delete\",\"create_index\"] } ] } POST _xpack/security/role/logstash_reader { \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"read\",\"view_index_metadata\"] } ] }","title":"Readme"},{"location":"Logstash/Readme/#configuring-logstash-on-an-elk-stack","text":"","title":"Configuring Logstash on an ELK Stack"},{"location":"Logstash/Readme/#pre-requisites","text":"Admin access to ELK Stack Logstash installed [refer to helm charts for chart] X-Pack enabled","title":"Pre Requisites"},{"location":"Logstash/Readme/#current-configuration-in-ops","text":"User - logstash-ingest Roles - Superadmin","title":"Current Configuration in Ops"},{"location":"Logstash/Readme/#steps-followed","text":"Create the following roles to be used by logstash user - logstash-ingest logstash_writer, logstash_reader POST _xpack/security/role/logstash_writer { \"cluster\": [\"manage_index_templates\", \"monitor\"], \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"write\",\"delete\",\"create_index\"] } ] } POST _xpack/security/role/logstash_reader { \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"read\",\"view_index_metadata\"] } ] }","title":"Steps followed"},{"location":"elasticsearch/Basic-Concepts/","text":"Elasticsearch Basic Concepts and Terms Near Real Time Elasticsearch is a near-realtime search platform. What this means is there is a slight latency (normally one second) from the time you index a document until the time it becomes searchable. Cluster A cluster is a collection of one or more nodes (servers) that together holds your entire data and provides federated indexing and search capabilities across all nodes. A cluster is identified by a unique name which by default is \"elasticsearch\". This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name. Make sure that you don\u2019t reuse the same cluster names in different environments, otherwise you might end up with nodes joining the wrong cluster. For instance you could use logging-dev, logging-stage, and logging-prod for the development, staging, and production clusters. Node A node is a single server that is part of your cluster, stores your data, and participates in the cluster\u2019s indexing and search capabilities. Just like a cluster, a node is identified by a name which by default is a random Universally Unique IDentifier (UUID) that is assigned to the node at startup. You can define any node name you want if you do not want the default. This name is important for administration purposes where you want to identify which servers in your network correspond to which nodes in your Elasticsearch cluster. A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named elasticsearch which means that if you start up a number of nodes on your network and\u2014assuming they can discover each other\u2014they will all automatically form and join a single cluster named elasticsearch. Index An index is a collection of documents that have somewhat similar characteristics. An index is identified by a name (that must be all lowercase) and this name is used to refer to the index when performing indexing, search, update, and delete operations against the documents in it. Type - Depreceated A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, e.g. one type for users, another type for blog posts. Reason - fields that have the same name in different mapping types are backed by the same Lucene field internally. This can lead to frustration when, for example, you want deleted to be a date field in one type and a boolean field in another type in the same index. On top of that, storing different entities that have few or no fields in common in the same index leads to sparse data and interferes with Lucene\u2019s ability to compress documents efficiently. Alternatives- 1. index per document type This approach has two benefits: Data is more likely to be dense and so benefit from compression techniques used in Lucene. The term statistics used for scoring in full text search are more likely to be accurate because all documents in the same index represent a single entity. custom type PUT twitter { \"mappings\": { \"_doc\": { \"properties\": { \"type\": { \"type\": \"keyword\" }, \"name\": { \"type\": \"text\" }, \"user_name\": { \"type\": \"keyword\" }, \"email\": { \"type\": \"keyword\" }, \"content\": { \"type\": \"text\" }, \"tweeted_at\": { \"type\": \"date\" } } } } } The explicit type field takes the place of the implicit _type field. Based on your search \"filter\": { \"match\": { \"type\": \"tweet\" or \"type\": \"user\" } } Refer - https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html Document - A document is a basic unit of information that can be indexed. This document is expressed in JSON (JavaScript Object Notation) which is a ubiquitous internet data interchange format. Shards and Replicas - Subdivide your index into multiple pieces called shards. When you create an index, you can simply define the number of shards that you want. Each shard is in itself a fully-functional and independent \"index\" that can be hosted on any node in the cluster. Sharding is important for two primary reasons: It allows you to horizontally split/scale your content volume It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput. Elasticsearch allows you to make one or more copies of your index\u2019s shards into what are called replica shards, or replicas for short. Replication is important for two primary reasons: It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from. It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel. The number of shards and replicas can be defined per index at the time the index is created. After the index is created, you may also change the number of replicas dynamically anytime. You can change the number of shards for an existing index using the _shrink and _split APIs, however this is not a trivial task and pre-planning for the correct number of shards is the optimal approach. Each Elasticsearch shard is a Lucene index. There is a maximum number of documents you can have in a single Lucene index. As of LUCENE-5843, the limit is 2,147,483,519 (= Integer.MAX_VALUE - 128) documents. You can monitor shard sizes using the _cat/shards API.","title":"Basic Concepts"},{"location":"elasticsearch/Basic-Concepts/#elasticsearch-basic-concepts-and-terms","text":"","title":"Elasticsearch Basic Concepts and Terms"},{"location":"elasticsearch/Basic-Concepts/#near-real-time","text":"Elasticsearch is a near-realtime search platform. What this means is there is a slight latency (normally one second) from the time you index a document until the time it becomes searchable.","title":"Near Real Time"},{"location":"elasticsearch/Basic-Concepts/#cluster","text":"A cluster is a collection of one or more nodes (servers) that together holds your entire data and provides federated indexing and search capabilities across all nodes. A cluster is identified by a unique name which by default is \"elasticsearch\". This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name. Make sure that you don\u2019t reuse the same cluster names in different environments, otherwise you might end up with nodes joining the wrong cluster. For instance you could use logging-dev, logging-stage, and logging-prod for the development, staging, and production clusters.","title":"Cluster"},{"location":"elasticsearch/Basic-Concepts/#node","text":"A node is a single server that is part of your cluster, stores your data, and participates in the cluster\u2019s indexing and search capabilities. Just like a cluster, a node is identified by a name which by default is a random Universally Unique IDentifier (UUID) that is assigned to the node at startup. You can define any node name you want if you do not want the default. This name is important for administration purposes where you want to identify which servers in your network correspond to which nodes in your Elasticsearch cluster. A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named elasticsearch which means that if you start up a number of nodes on your network and\u2014assuming they can discover each other\u2014they will all automatically form and join a single cluster named elasticsearch.","title":"Node"},{"location":"elasticsearch/Basic-Concepts/#index","text":"An index is a collection of documents that have somewhat similar characteristics. An index is identified by a name (that must be all lowercase) and this name is used to refer to the index when performing indexing, search, update, and delete operations against the documents in it.","title":"Index"},{"location":"elasticsearch/Basic-Concepts/#type-depreceated","text":"A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, e.g. one type for users, another type for blog posts. Reason - fields that have the same name in different mapping types are backed by the same Lucene field internally. This can lead to frustration when, for example, you want deleted to be a date field in one type and a boolean field in another type in the same index. On top of that, storing different entities that have few or no fields in common in the same index leads to sparse data and interferes with Lucene\u2019s ability to compress documents efficiently. Alternatives- 1. index per document type This approach has two benefits: Data is more likely to be dense and so benefit from compression techniques used in Lucene. The term statistics used for scoring in full text search are more likely to be accurate because all documents in the same index represent a single entity. custom type PUT twitter { \"mappings\": { \"_doc\": { \"properties\": { \"type\": { \"type\": \"keyword\" }, \"name\": { \"type\": \"text\" }, \"user_name\": { \"type\": \"keyword\" }, \"email\": { \"type\": \"keyword\" }, \"content\": { \"type\": \"text\" }, \"tweeted_at\": { \"type\": \"date\" } } } } } The explicit type field takes the place of the implicit _type field. Based on your search \"filter\": { \"match\": { \"type\": \"tweet\" or \"type\": \"user\" } } Refer - https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html","title":"Type - Depreceated"},{"location":"elasticsearch/Basic-Concepts/#document-","text":"A document is a basic unit of information that can be indexed. This document is expressed in JSON (JavaScript Object Notation) which is a ubiquitous internet data interchange format.","title":"Document -"},{"location":"elasticsearch/Basic-Concepts/#shards-and-replicas-","text":"Subdivide your index into multiple pieces called shards. When you create an index, you can simply define the number of shards that you want. Each shard is in itself a fully-functional and independent \"index\" that can be hosted on any node in the cluster. Sharding is important for two primary reasons: It allows you to horizontally split/scale your content volume It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput. Elasticsearch allows you to make one or more copies of your index\u2019s shards into what are called replica shards, or replicas for short. Replication is important for two primary reasons: It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from. It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel. The number of shards and replicas can be defined per index at the time the index is created. After the index is created, you may also change the number of replicas dynamically anytime. You can change the number of shards for an existing index using the _shrink and _split APIs, however this is not a trivial task and pre-planning for the correct number of shards is the optimal approach. Each Elasticsearch shard is a Lucene index. There is a maximum number of documents you can have in a single Lucene index. As of LUCENE-5843, the limit is 2,147,483,519 (= Integer.MAX_VALUE - 128) documents. You can monitor shard sizes using the _cat/shards API.","title":"Shards and Replicas -"},{"location":"elasticsearch/Exploring-Cluster/","text":"Cluster health GET /_cat/health?v pattern of how we access data in Elasticsearch. That pattern can be summarized as follows: <HTTP Verb> /<Index>/<Type>/<ID> cluster health, we either get green, yellow, or red. Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) Note: When a cluster is red, it will continue to serve search requests from the available shards but you will likely need to fix it ASAP since there are unassigned shards. Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html Cluster state updates The master node is the only node in a cluster that can make changes to the cluster state. The master node processes one cluster state update at a time, applies the required changes and publishes the updated cluster state to all the other nodes in the cluster. Each node receives the publish message, acknowledges it, but does not yet apply it. If the master does not receive acknowledgement from at least discovery.zen.minimum_master_nodes nodes within a certain time (controlled by the discovery.zen.commit_timeout setting and defaults to 30 seconds) the cluster state change is rejected. Once enough nodes have responded, the cluster state is committed and a message will be sent to all the nodes. The nodes then proceed to apply the new cluster state to their internal state. The master node waits for all nodes to respond, up to a timeout, before going ahead processing the next updates in the queue. The discovery.zen.publish_timeout is set by default to 30 seconds and is measured from the moment the publishing started. Both timeout settings can be changed dynamically through the cluster update settings api. https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html","title":"Exploring Cluster"},{"location":"elasticsearch/Exploring-Cluster/#cluster-health","text":"GET /_cat/health?v pattern of how we access data in Elasticsearch. That pattern can be summarized as follows: <HTTP Verb> /<Index>/<Type>/<ID> cluster health, we either get green, yellow, or red. Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) Note: When a cluster is red, it will continue to serve search requests from the available shards but you will likely need to fix it ASAP since there are unassigned shards. Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html Cluster state updates The master node is the only node in a cluster that can make changes to the cluster state. The master node processes one cluster state update at a time, applies the required changes and publishes the updated cluster state to all the other nodes in the cluster. Each node receives the publish message, acknowledges it, but does not yet apply it. If the master does not receive acknowledgement from at least discovery.zen.minimum_master_nodes nodes within a certain time (controlled by the discovery.zen.commit_timeout setting and defaults to 30 seconds) the cluster state change is rejected. Once enough nodes have responded, the cluster state is committed and a message will be sent to all the nodes. The nodes then proceed to apply the new cluster state to their internal state. The master node waits for all nodes to respond, up to a timeout, before going ahead processing the next updates in the queue. The discovery.zen.publish_timeout is set by default to 30 seconds and is measured from the moment the publishing started. Both timeout settings can be changed dynamically through the cluster update settings api. https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html","title":"Cluster health"},{"location":"elasticsearch/Finding-your-feet/","text":"Finding your feet Lets start with a simple tutorial covering basics concepts like indexing, search and aggregations. We will cover more on them in next sections. Have a running instance of Elasticsearch running, single node cluster will be good to follow along. For details on the different installations you can refer this getting-started-install . For keeping things simple we will use docker to run the elasticsearch node and later on use docker-compose to have a working ELK Stack. To start a single-node ES cluster for development or testing docker run -p 9200 :9200 -p 9300 :9300 \\ -e \"discovery.type=single-node\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.10.1 Index some documents PUT /customer/_doc/1 { \"name\" : \"John Doe\" } This request automatically creates the customer index if it doesn\u2019t already exist, adds a new document that has an ID of 1, and stores and indexes the name field. The new document is available immediately from any node in the cluster. You can retrieve it with a GET request that specifies its document ID. GET /customer/_doc/1 You can also load documents in bulk by following steps- Download the accounts.json sample data set. Index the account data into the bank index with the following _bulk request: ## Load the bulk json curl -H \"Content-Type: application/json\" \\ -XPOST \"localhost:9200/bank/_bulk?pretty&refresh\" \\ --data-binary \"@accounts.json\" ## check the new index with the documents curl \"localhost:9200/_cat/indices?v\" The response indicates that 1,000 documents were indexed successfully. Start Searching Once you have ingested some data into an Elasticsearch index, you can search it by sending requests to the _search endpoint. - To access the full suite of search capabilities, you use the Elasticsearch Query DSL to specify the search criteria in the request body. We will discuss more on this in the later sections. - You can specify the name of the index you want to search in the request URI. GET /bank/_search { \"query\" : { \"match_all\" : {} } , \"sort\" : [ { \"account_number\" : \"asc\" } ] } Each search request is self-contained : meaning Elasticsearch does not maintain any state information across requests, so every request you fire will be a new request all together. To page through the search hits, specify the from and size parameters in your request. For example, the following request gets hits 10 through 19: GET /bank/_search { \"query\" : { \"match_all\" : {} } , \"sort\" : [ { \"account_number\" : \"asc\" } ] , \"from\" : 10 , \"size\" : 10 } Full-Text Search - the following request only matches addresses that contain the phrase mill lane GET /bank/_search { \"query\" : { \"match_phrase\" : { \"address\" : \"mill lane\" } } } Analytics Elasticsearch aggregations enable you to get meta-information about your search results and answer questions like, \"How many account holders are in Texas?\" or \"What\u2019s the average balance of accounts in Tennessee?\" You can search documents, filter hits, and use aggregations to analyze the results all in one request. GET /bank/_search { \"size\" : 0 , \"aggs\" : { \"group_by_state\" : { \"terms\" : { \"field\" : \"state.keyword\" } } } } Next Steps Now that you\u2019ve set up a cluster, indexed some documents, and run some searches and aggregations, time to dig inside elasticsearch more.","title":"Finding your feet"},{"location":"elasticsearch/Finding-your-feet/#finding-your-feet","text":"Lets start with a simple tutorial covering basics concepts like indexing, search and aggregations. We will cover more on them in next sections. Have a running instance of Elasticsearch running, single node cluster will be good to follow along. For details on the different installations you can refer this getting-started-install . For keeping things simple we will use docker to run the elasticsearch node and later on use docker-compose to have a working ELK Stack. To start a single-node ES cluster for development or testing docker run -p 9200 :9200 -p 9300 :9300 \\ -e \"discovery.type=single-node\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.10.1","title":"Finding your feet"},{"location":"elasticsearch/Finding-your-feet/#index-some-documents","text":"PUT /customer/_doc/1 { \"name\" : \"John Doe\" } This request automatically creates the customer index if it doesn\u2019t already exist, adds a new document that has an ID of 1, and stores and indexes the name field. The new document is available immediately from any node in the cluster. You can retrieve it with a GET request that specifies its document ID. GET /customer/_doc/1 You can also load documents in bulk by following steps- Download the accounts.json sample data set. Index the account data into the bank index with the following _bulk request: ## Load the bulk json curl -H \"Content-Type: application/json\" \\ -XPOST \"localhost:9200/bank/_bulk?pretty&refresh\" \\ --data-binary \"@accounts.json\" ## check the new index with the documents curl \"localhost:9200/_cat/indices?v\" The response indicates that 1,000 documents were indexed successfully.","title":"Index some documents"},{"location":"elasticsearch/Finding-your-feet/#start-searching","text":"Once you have ingested some data into an Elasticsearch index, you can search it by sending requests to the _search endpoint. - To access the full suite of search capabilities, you use the Elasticsearch Query DSL to specify the search criteria in the request body. We will discuss more on this in the later sections. - You can specify the name of the index you want to search in the request URI. GET /bank/_search { \"query\" : { \"match_all\" : {} } , \"sort\" : [ { \"account_number\" : \"asc\" } ] } Each search request is self-contained : meaning Elasticsearch does not maintain any state information across requests, so every request you fire will be a new request all together. To page through the search hits, specify the from and size parameters in your request. For example, the following request gets hits 10 through 19: GET /bank/_search { \"query\" : { \"match_all\" : {} } , \"sort\" : [ { \"account_number\" : \"asc\" } ] , \"from\" : 10 , \"size\" : 10 } Full-Text Search - the following request only matches addresses that contain the phrase mill lane GET /bank/_search { \"query\" : { \"match_phrase\" : { \"address\" : \"mill lane\" } } }","title":"Start Searching"},{"location":"elasticsearch/Finding-your-feet/#analytics","text":"Elasticsearch aggregations enable you to get meta-information about your search results and answer questions like, \"How many account holders are in Texas?\" or \"What\u2019s the average balance of accounts in Tennessee?\" You can search documents, filter hits, and use aggregations to analyze the results all in one request. GET /bank/_search { \"size\" : 0 , \"aggs\" : { \"group_by_state\" : { \"terms\" : { \"field\" : \"state.keyword\" } } } }","title":"Analytics"},{"location":"elasticsearch/Finding-your-feet/#next-steps","text":"Now that you\u2019ve set up a cluster, indexed some documents, and run some searches and aggregations, time to dig inside elasticsearch more.","title":"Next Steps"},{"location":"elasticsearch/Modifying-Data/","text":"Updating Documnents Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. POST /customer/_doc/1/_update?pretty { \"doc\": { \"name\": \"Jane Doe\" } } Batch Processing In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as few network roundtrips as possible. The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.","title":"Modifying Data"},{"location":"elasticsearch/Modifying-Data/#updating-documnents","text":"Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. POST /customer/_doc/1/_update?pretty { \"doc\": { \"name\": \"Jane Doe\" } }","title":"Updating Documnents"},{"location":"elasticsearch/Modifying-Data/#batch-processing","text":"In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as few network roundtrips as possible. The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.","title":"Batch Processing"},{"location":"elasticsearch/What-is-Elasticsearch/","text":"What is Elasticsearch ? In simplest terms we need a search engine or technology to easily retrieve the relevant result in the shortest amount of time. Few personal areas I think any search engine should answer How can I search the documents ? How can I analyse them and take out useful data out of it ? How it can be scalable ? Ease of use and community support. There are many tools right now that can help you with the above points, one of the tools which the community and enterprise are using is Elasticsearch. Elasticsearch is an open-source search engine built on top of Apache Lucene, which is a full-text search-engine library. Thing to notice Lucene is just a library, to leverage its power, we need to work in Java integrate it directly with the application. Lucene is very complex and this approach is time and resource consuming. Elasticsearch to the rescue Elasticsearch, written in Java which uses Lucene internally for all of its indexing and searching, it aims to make full-text search easy by hiding the complexities of Lucene behind a simple, coherent, RESTful API. However Elasticsearch is more then \"just\" full-text search. It can also be described as follows: - A distributed near real-time document store where every field is indexed and searchable. - A distributed search engine with real-time analytics - Capable of scaling to hundreds of servers and petabytes of structured and unstructured data Moreover it packages up all this functionality into a standalone server that any application can talk to via a simple RESTful API , using any webclient or just cli or even simple curl commands REST-API with JSON over HTTP Having an API endpoint exposed (default 9200) any client can communicate with Elasticsearch. A request to Elasticsearch consists of the same parts as any HTTP request: curl -X<VERB> '<PROTOCOL>://<HOST>/<PATH>?<QUERY_STRING>' -d '<BODY>' where VERB : The appropriate HTTP method or verb: GET, POST, PUT, HEAD, or DELETE. PROTOCOL : Either http or https HOST : The hostname of any node in your Elasticsearch cluster, or localhost for a node on your local machine. PORT : The port running the Elasticsearch HTTP service, which defaults to 9200. QUERY_STRING : Any optional query-string parameters BODY : A JSON-encoded request body Example - to count the number of documents in the cluster curl -XGET 'http://localhost:9200/_count?pretty' -d ' { \"query\": { \"match_all\": {} } }' Document Oriented Elasticsearch is document oriented, meaning that it stores entire objects as documents . In simpler terms any message, event you send to Elasticsearch is saved as a Document in JSON format. Also ES not only stores them, but also indexes the contents of each document in order to make them searchable. In short you index, search, sort, and filter documents \u2014 not rows of columnar data. Note Elasticsearch uses JavaScript Object Notation, or JSON , as the serialization format for documents. JSON serialization is supported by most programming languages, and has become the standard format used by the NoSQL movement. Closing points The above points can help you understanding the rational behind Elasticsearch, but with any technology it doesn't give justice without an example. Check the next section Finding Your Feet for a easy to start example.","title":"What is Elasticsearch"},{"location":"elasticsearch/What-is-Elasticsearch/#what-is-elasticsearch","text":"In simplest terms we need a search engine or technology to easily retrieve the relevant result in the shortest amount of time. Few personal areas I think any search engine should answer How can I search the documents ? How can I analyse them and take out useful data out of it ? How it can be scalable ? Ease of use and community support. There are many tools right now that can help you with the above points, one of the tools which the community and enterprise are using is Elasticsearch. Elasticsearch is an open-source search engine built on top of Apache Lucene, which is a full-text search-engine library. Thing to notice Lucene is just a library, to leverage its power, we need to work in Java integrate it directly with the application. Lucene is very complex and this approach is time and resource consuming. Elasticsearch to the rescue Elasticsearch, written in Java which uses Lucene internally for all of its indexing and searching, it aims to make full-text search easy by hiding the complexities of Lucene behind a simple, coherent, RESTful API. However Elasticsearch is more then \"just\" full-text search. It can also be described as follows: - A distributed near real-time document store where every field is indexed and searchable. - A distributed search engine with real-time analytics - Capable of scaling to hundreds of servers and petabytes of structured and unstructured data Moreover it packages up all this functionality into a standalone server that any application can talk to via a simple RESTful API , using any webclient or just cli or even simple curl commands","title":"What is Elasticsearch ?"},{"location":"elasticsearch/What-is-Elasticsearch/#rest-api-with-json-over-http","text":"Having an API endpoint exposed (default 9200) any client can communicate with Elasticsearch. A request to Elasticsearch consists of the same parts as any HTTP request: curl -X<VERB> '<PROTOCOL>://<HOST>/<PATH>?<QUERY_STRING>' -d '<BODY>' where VERB : The appropriate HTTP method or verb: GET, POST, PUT, HEAD, or DELETE. PROTOCOL : Either http or https HOST : The hostname of any node in your Elasticsearch cluster, or localhost for a node on your local machine. PORT : The port running the Elasticsearch HTTP service, which defaults to 9200. QUERY_STRING : Any optional query-string parameters BODY : A JSON-encoded request body Example - to count the number of documents in the cluster curl -XGET 'http://localhost:9200/_count?pretty' -d ' { \"query\": { \"match_all\": {} } }'","title":"REST-API with JSON over HTTP"},{"location":"elasticsearch/What-is-Elasticsearch/#document-oriented","text":"Elasticsearch is document oriented, meaning that it stores entire objects as documents . In simpler terms any message, event you send to Elasticsearch is saved as a Document in JSON format. Also ES not only stores them, but also indexes the contents of each document in order to make them searchable. In short you index, search, sort, and filter documents \u2014 not rows of columnar data. Note Elasticsearch uses JavaScript Object Notation, or JSON , as the serialization format for documents. JSON serialization is supported by most programming languages, and has become the standard format used by the NoSQL movement.","title":"Document Oriented"},{"location":"elasticsearch/What-is-Elasticsearch/#closing-points","text":"The above points can help you understanding the rational behind Elasticsearch, but with any technology it doesn't give justice without an example. Check the next section Finding Your Feet for a easy to start example.","title":"Closing points"},{"location":"elasticsearch-certification/Readme/","text":"Few important points to note in the exam Elastic search version: 7.2 Certification Topics Installation and Configuration Deploy and start an Elasticsearch cluster that satisfies a given set of requirements Configure the nodes of a cluster to satisfy a given set of requirements Secure a cluster using Elasticsearch Security Define role-based access control using Elasticsearch Security Indexing Data Define an index that satisfies a given set of requirements Perform index, create, read, update, and delete operations on the documents of an index Define and use index aliases Define and use an index template for a given pattern that satisfies a given set of requirements Define and use a dynamic template that satisfies a given set of requirements Use the Reindex API and Update By Query API to reindex and/or update documents Define and use an ingest pipeline that satisfies a given set of requirements, including the use of Painless to modify documents Queries Write and execute a search query for terms and/or phrases in one or more fields of an index Write and execute a search query that is a Boolean combination of multiple queries and filters Highlight the search terms in the response of a query Sort the results of a query by a given set of requirements Implement pagination of the results of a search query Apply fuzzy matching to a query Define and use a search template Write and execute a query that searches across multiple clusters Aggregations Write and execute metric and bucket aggregations Write and execute aggregations that contain sub-aggregations Mappings and Text Analysis Define a mapping that satisfies a given set of requirements Define and use a custom analyzer that satisfies a given set of requirements Define and use multi-fields with different data types and/or analyzers Configure an index so that it properly maintains the relationships of nested arrays of objects Cluster Administration Allocate the shards of an index to specific nodes based on a given set of requirements Configure shard allocation awareness and forced awareness for an index Diagnose shard issues and repair a cluster's health Backup and restore a cluster and/or specific indices Configure a cluster for use with a hot/warm architecture Configure a cluster for cross cluster search","title":"Overview"},{"location":"elasticsearch-certification/Readme/#few-important-points-to-note-in-the-exam","text":"Elastic search version: 7.2","title":"Few important points to note in the exam"},{"location":"elasticsearch-certification/Readme/#certification-topics","text":"Installation and Configuration Deploy and start an Elasticsearch cluster that satisfies a given set of requirements Configure the nodes of a cluster to satisfy a given set of requirements Secure a cluster using Elasticsearch Security Define role-based access control using Elasticsearch Security Indexing Data Define an index that satisfies a given set of requirements Perform index, create, read, update, and delete operations on the documents of an index Define and use index aliases Define and use an index template for a given pattern that satisfies a given set of requirements Define and use a dynamic template that satisfies a given set of requirements Use the Reindex API and Update By Query API to reindex and/or update documents Define and use an ingest pipeline that satisfies a given set of requirements, including the use of Painless to modify documents Queries Write and execute a search query for terms and/or phrases in one or more fields of an index Write and execute a search query that is a Boolean combination of multiple queries and filters Highlight the search terms in the response of a query Sort the results of a query by a given set of requirements Implement pagination of the results of a search query Apply fuzzy matching to a query Define and use a search template Write and execute a query that searches across multiple clusters Aggregations Write and execute metric and bucket aggregations Write and execute aggregations that contain sub-aggregations Mappings and Text Analysis Define a mapping that satisfies a given set of requirements Define and use a custom analyzer that satisfies a given set of requirements Define and use multi-fields with different data types and/or analyzers Configure an index so that it properly maintains the relationships of nested arrays of objects Cluster Administration Allocate the shards of an index to specific nodes based on a given set of requirements Configure shard allocation awareness and forced awareness for an index Diagnose shard issues and repair a cluster's health Backup and restore a cluster and/or specific indices Configure a cluster for use with a hot/warm architecture Configure a cluster for cross cluster search","title":"Certification Topics"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/","text":"Deploying an ES Cluster Topics covered here Prepare a Linux system for Elasticsearch Deploy Elasticsearch from and archive Configure a multi-node cluster Add custom node attributes Bind Elasticsearch to specific addresses Configure node roles Name a cluster Name each node in a cluster Configure node discovery Configure a single-node cluster Start Elasticsearch Deploy Kibana Configure Kibana Start Kibana Prepare a Linux system for Elasticsearch We will be deploying two ES cluster with the following specification Cluster-1: total 3 nodes - 1 Master + Kibana Node - 2 Data Nodes Cluster-2: total 1 nodes - 1 Master + Kibana Node Why two clusters, this is to demonstrate Cross Cluster search functionality Cross Cluster Replication Backups and Restore Operations Architecture The below will be the architecture of the cluster we will be creating. Implementation For implementing the above architecture we will follow the following approaches: Cloud Based VMs Spin up 4 small nodes with 2g memory, do check u can ssh into each of the nodes. The Vagrant Based VMs Will be covered in the next sections. Deploy Elasticsearch from and archive create a user \"elastic\" Disabling the swap space file descriptors or file handles increased, \"elastic - nofile 65536\" >> add this to /etc/security/limits.conf Virtual memory increased, \"vm.max_map_count = 262144\" >> /etc/sysctl.conf Increase number of threads, \"elastic - nproc 4096\" >> add this to /etc/security/limits.conf Address space to unlimited [Part of Bootstrap check] Run the file with sudo privileges #!/bin/bash useradd elastic swapoff -a # sed to comment the swap partition in /etc/fstab and keep a backup sed -i.bak -r 's/(.+ swap .+)/#\\1/' /etc/fstab echo \"elastic - nofile 65536\" >> /etc/security/limits.conf echo \"vm.max_map_count = 262144\" >> /etc/sysctl.conf echo \"elastic - nproc 4096\" >> /etc/security/limits.conf # Refresh the sysctl sysctl -p # run the command as elastic user sudo -u elastic -H sh -c \" cd /home/elastic; curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.1-linux-x86_64.tar.gz; tar -xzf elasticsearch-7.2.1-linux-x86_64.tar.gz mv elasticsearch-7.2.1 elasticsearch rm -rf elasticsearch-7.2.1-linux-x86_64.tar.gz \" After having the installation, time to configure elasticsearch on each node, as per the architecture diagram. Configure a multi-node cluster Elasticsearch Config yaml to be copied as per the node. For cluster-1 # master node cluster.name: cluster-1 node.name: master-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"master-1\"] node: master: true data: false ingest: false # data node - 1 cluster.name: cluster-1 node.name: data-1 node.attr.zone: 1 node.attr.temp: hot network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] # tell the discovery hosts, private ip of master nodes cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false # data node-2 cluster.name: cluster-1 node.name: data-2 node.attr.zone: 1 node.attr.temp: hot network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] # tell the discovery hosts, private ip of master nodes cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false Configure a single-node cluster For cluster-2 # single node cluster cluster.name: cluster-2 node.name: node-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"node-1\"] node: master: true data: true ingest: true Note Update jvm.options file with the correct memory settings, 1g is the default. Start the elasticsearch Run from the elasticsearch folder ./bin/elasticsearch -d -p pid this will give u the process pid which u can use to kill the process. check the nodes are connected by checking logs or running the command curl http://localhost:9200/_cat/nodes?v Install kibana Run the following command on master-1 node curl -O https://artifacts.elastic.co/downloads/kibana/kibana-7.2.1-linux-x86_64.tar.gz ; tar -xzf kibana-7.2.1-linux-x86_64.tar.gz Start kibana From directory in which you installed kibana ./bin/kibana","title":"Deploying ES Cluster"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#deploying-an-es-cluster","text":"Topics covered here Prepare a Linux system for Elasticsearch Deploy Elasticsearch from and archive Configure a multi-node cluster Add custom node attributes Bind Elasticsearch to specific addresses Configure node roles Name a cluster Name each node in a cluster Configure node discovery Configure a single-node cluster Start Elasticsearch Deploy Kibana Configure Kibana Start Kibana","title":"Deploying an ES Cluster"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#prepare-a-linux-system-for-elasticsearch","text":"We will be deploying two ES cluster with the following specification Cluster-1: total 3 nodes - 1 Master + Kibana Node - 2 Data Nodes Cluster-2: total 1 nodes - 1 Master + Kibana Node Why two clusters, this is to demonstrate Cross Cluster search functionality Cross Cluster Replication Backups and Restore Operations","title":"Prepare a Linux system for Elasticsearch"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#architecture","text":"The below will be the architecture of the cluster we will be creating.","title":"Architecture"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#implementation","text":"For implementing the above architecture we will follow the following approaches: Cloud Based VMs Spin up 4 small nodes with 2g memory, do check u can ssh into each of the nodes. The Vagrant Based VMs Will be covered in the next sections.","title":"Implementation"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#deploy-elasticsearch-from-and-archive","text":"create a user \"elastic\" Disabling the swap space file descriptors or file handles increased, \"elastic - nofile 65536\" >> add this to /etc/security/limits.conf Virtual memory increased, \"vm.max_map_count = 262144\" >> /etc/sysctl.conf Increase number of threads, \"elastic - nproc 4096\" >> add this to /etc/security/limits.conf Address space to unlimited [Part of Bootstrap check] Run the file with sudo privileges #!/bin/bash useradd elastic swapoff -a # sed to comment the swap partition in /etc/fstab and keep a backup sed -i.bak -r 's/(.+ swap .+)/#\\1/' /etc/fstab echo \"elastic - nofile 65536\" >> /etc/security/limits.conf echo \"vm.max_map_count = 262144\" >> /etc/sysctl.conf echo \"elastic - nproc 4096\" >> /etc/security/limits.conf # Refresh the sysctl sysctl -p # run the command as elastic user sudo -u elastic -H sh -c \" cd /home/elastic; curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.1-linux-x86_64.tar.gz; tar -xzf elasticsearch-7.2.1-linux-x86_64.tar.gz mv elasticsearch-7.2.1 elasticsearch rm -rf elasticsearch-7.2.1-linux-x86_64.tar.gz \" After having the installation, time to configure elasticsearch on each node, as per the architecture diagram.","title":"Deploy Elasticsearch from and archive"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#configure-a-multi-node-cluster","text":"Elasticsearch Config yaml to be copied as per the node. For cluster-1 # master node cluster.name: cluster-1 node.name: master-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"master-1\"] node: master: true data: false ingest: false # data node - 1 cluster.name: cluster-1 node.name: data-1 node.attr.zone: 1 node.attr.temp: hot network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] # tell the discovery hosts, private ip of master nodes cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false # data node-2 cluster.name: cluster-1 node.name: data-2 node.attr.zone: 1 node.attr.temp: hot network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] # tell the discovery hosts, private ip of master nodes cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false","title":"Configure a multi-node cluster"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#configure-a-single-node-cluster","text":"For cluster-2 # single node cluster cluster.name: cluster-2 node.name: node-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"node-1\"] node: master: true data: true ingest: true Note Update jvm.options file with the correct memory settings, 1g is the default.","title":"Configure a single-node cluster"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#start-the-elasticsearch","text":"Run from the elasticsearch folder ./bin/elasticsearch -d -p pid this will give u the process pid which u can use to kill the process. check the nodes are connected by checking logs or running the command curl http://localhost:9200/_cat/nodes?v","title":"Start the elasticsearch"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#install-kibana","text":"Run the following command on master-1 node curl -O https://artifacts.elastic.co/downloads/kibana/kibana-7.2.1-linux-x86_64.tar.gz ; tar -xzf kibana-7.2.1-linux-x86_64.tar.gz","title":"Install kibana"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Deploying-ES-Cluster/#start-kibana","text":"From directory in which you installed kibana ./bin/kibana","title":"Start kibana"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Outline/","text":"Topics covered Deploy and start an Elasticsearch cluster that satisfies a given set of requirements. Configure the nodes of a cluster to satisfy a given set of requirements Secure a cluster using Elasticsearch Security Define role-based access control using Elasticsearch Security","title":"Outline"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Outline/#topics-covered","text":"Deploy and start an Elasticsearch cluster that satisfies a given set of requirements. Configure the nodes of a cluster to satisfy a given set of requirements Secure a cluster using Elasticsearch Security Define role-based access control using Elasticsearch Security","title":"Topics covered"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/RBAC-in-ES/","text":"Define Role-Based Access Control Using Elasticsearch Security With Elasticsearch Security, you can create highly granular roles to permit or restrict access to specific indexes, documents, and fields. Follow along on the Linux Academy Cloud Playground servers as we demonstrate how to: Use Kibana's management UI to create roles and users Use Elasticsearch APIs to create roles and users Limit access to specific cluster permissions Limit access to specific indexes Only allow specific actions on indexes Create a user with a given set of roles","title":"Define Role-Based Access Control Using Elasticsearch Security"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/RBAC-in-ES/#define-role-based-access-control-using-elasticsearch-security","text":"With Elasticsearch Security, you can create highly granular roles to permit or restrict access to specific indexes, documents, and fields. Follow along on the Linux Academy Cloud Playground servers as we demonstrate how to: Use Kibana's management UI to create roles and users Use Elasticsearch APIs to create roles and users Limit access to specific cluster permissions Limit access to specific indexes Only allow specific actions on indexes Create a user with a given set of roles","title":"Define Role-Based Access Control Using Elasticsearch Security"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/","text":"Securing the Elastic Search cluster Without the Security plugin, Elasticsearch is susceptible to unauthorized use by nefarious actors. To secure Elasticsearch properly, you must use Elasticsearch Security to encrypt the various networks and enforce granular, role-based user access control. I encourage you to follow along on the Linux Academy Cloud Playground servers as we demonstrate how to: Generate a CA with the certutil tool Generate node certificates Add certificate passwords to the Elasticsearch keystore Encrypt the transport network Set built-in user passwords Encrypt the HTTP network Configure Kibana to work with a secured Elasticsearch cluster Generating certificates for each Nodes /home/elastic/elasticsearch/bin/elasticsearch-certutil ca --out config/certs/ca --pass elastic ## master, data and node-1 nodes using the same CA /home/elastic/elasticsearch/bin/elasticsearch-certutil cert --ca config/certs/ca --ca-pass elastic --name node-1 --out config/certs/node-1 --pass elastic Note certificates generated are relative to the elasticsearch installation directory, which in our case in /home/elastic/elasticsearch, so conf dir is the one present here. TODO: Full host verification with IP and Hostname, also the format of certificates is PKCS#12, try with PEM too cd /home/elastic/elasticsearch echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.transport.ssl.keystore.secure_password echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.transport.ssl.truststore.secure_password echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.http.ssl.keystore.secure_password echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.http.ssl.truststore.secure_password New ES Files cluster.name: cluster-1 node.name: master-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"master-1\"] node: master: true data: false ingest: false xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/master-1 truststore: path: certs/master-1 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/master-1 truststore: path: certs/master-1 ## data-1 node cluster.name: cluster-1 node.name: data-1 node.attr.zone: 1 node.attr.temp: hot network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-1 truststore: path: certs/data-1 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-1 truststore: path: certs/data-1 ## data-2 node cluster.name: cluster-1 node.name: data-2 node.attr.zone: 2 node.attr.temp: warm network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-2 truststore: path: certs/data-2 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-2 truststore: path: certs/data-2 ## cluster-2 node-1 cluster.name: cluster-2 node.name: node-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"node-1\"] node: master: true data: true ingest: true xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/node-1 truststore: path: certs/node-1 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/node-1 truststore: path: certs/node-1 Restart commands pkill -F pid && ./bin/elasticsearch -d -p pid setup passwords ./bin/elasticsearch-setup-passwords interactive # change in kibana yaml with username and password","title":"Securing ES Cluster"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/#securing-the-elastic-search-cluster","text":"Without the Security plugin, Elasticsearch is susceptible to unauthorized use by nefarious actors. To secure Elasticsearch properly, you must use Elasticsearch Security to encrypt the various networks and enforce granular, role-based user access control. I encourage you to follow along on the Linux Academy Cloud Playground servers as we demonstrate how to: Generate a CA with the certutil tool Generate node certificates Add certificate passwords to the Elasticsearch keystore Encrypt the transport network Set built-in user passwords Encrypt the HTTP network Configure Kibana to work with a secured Elasticsearch cluster","title":"Securing the Elastic Search cluster"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/#generating-certificates-for-each-nodes","text":"/home/elastic/elasticsearch/bin/elasticsearch-certutil ca --out config/certs/ca --pass elastic ## master, data and node-1 nodes using the same CA /home/elastic/elasticsearch/bin/elasticsearch-certutil cert --ca config/certs/ca --ca-pass elastic --name node-1 --out config/certs/node-1 --pass elastic Note certificates generated are relative to the elasticsearch installation directory, which in our case in /home/elastic/elasticsearch, so conf dir is the one present here.","title":"Generating certificates for each Nodes"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/#todo-full-host-verification-with-ip-and-hostname-also-the-format-of-certificates-is-pkcs12-try-with-pem-too","text":"cd /home/elastic/elasticsearch echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.transport.ssl.keystore.secure_password echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.transport.ssl.truststore.secure_password echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.http.ssl.keystore.secure_password echo \"elastic\" | ./bin/elasticsearch-keystore add --stdin xpack.security.http.ssl.truststore.secure_password","title":"TODO: Full host verification with IP and Hostname, also the format of certificates is PKCS#12, try with PEM too"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/#new-es-files","text":"cluster.name: cluster-1 node.name: master-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"master-1\"] node: master: true data: false ingest: false xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/master-1 truststore: path: certs/master-1 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/master-1 truststore: path: certs/master-1 ## data-1 node cluster.name: cluster-1 node.name: data-1 node.attr.zone: 1 node.attr.temp: hot network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-1 truststore: path: certs/data-1 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-1 truststore: path: certs/data-1 ## data-2 node cluster.name: cluster-1 node.name: data-2 node.attr.zone: 2 node.attr.temp: warm network.host: [_local_, _site_] discovery.seed_hosts: [\"172.31.30.103\"] cluster.initial_master_nodes: [\"master-1\"] node: master: false data: true ingest: false xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-2 truststore: path: certs/data-2 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/data-2 truststore: path: certs/data-2 ## cluster-2 node-1 cluster.name: cluster-2 node.name: node-1 node.attr.zone: 1 network.host: [_local_, _site_] cluster.initial_master_nodes: [\"node-1\"] node: master: true data: true ingest: true xpack.security: enabled: true transport: ssl: enabled: true verification_mode: certificate keystore: path: certs/node-1 truststore: path: certs/node-1 http: ssl: enabled: true verification_mode: certificate keystore: path: certs/node-1 truststore: path: certs/node-1","title":"New ES Files"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/#restart-commands","text":"pkill -F pid && ./bin/elasticsearch -d -p pid","title":"Restart commands"},{"location":"elasticsearch-certification/01-Installation-and-Configuration/Securing-ES-Cluster/#setup-passwords","text":"./bin/elasticsearch-setup-passwords interactive # change in kibana yaml with username and password","title":"setup passwords"},{"location":"indexing-data/Aliases/","text":"ES Aliases POST _aliases { \"actions\" : [ { \"add\" : { \"index\" : \"sample-1\" , \"alias\" : \"dummy\" } } ] } POST API as this will update the index alias <logstash-{now/d}-000001>","title":"Aliases"},{"location":"indexing-data/Aliases/#es-aliases","text":"POST _aliases { \"actions\" : [ { \"add\" : { \"index\" : \"sample-1\" , \"alias\" : \"dummy\" } } ] } POST API as this will update the index alias <logstash-{now/d}-000001>","title":"ES Aliases"},{"location":"indexing-data/CRUD-Operations/","text":"CRUD Operations on a ES Index # crea te a n i n dex PUT sample { \"settings\" : { \"number_of_replicas\" : 1 , \"number_of_shards\" : 1 } } # Crea te some da ta i nt o t he i n dex PUT sample/_doc/ 1 { \"firstname\" : \"gaurav\" , \"lastname\" : \"sood\" } # Read t he da ta fr om t he i n dex GET sample/_doc/ 1 # Upda te t he da ta i n t he i n dex POST sample/_upda te / 1 { \"doc\" : { \"middlename\" : \"kumar\" } } # or usi n g scrip te d pai nless POST sample/_upda te / 1 { \"script\" : { \"lang\" : \"painless\" , \"source\" : \"ctx._source.remove('middlename')\" } } # DELETE doc a n d i n dex DELETE sample -1 /_doc/ 1 DELETE sample -1 From this understanding about APIs, - PUT operation is used to create a new objects - POST operation is used to update the objects","title":"CRUD Operations"},{"location":"indexing-data/CRUD-Operations/#crud-operations-on-a-es-index","text":"# crea te a n i n dex PUT sample { \"settings\" : { \"number_of_replicas\" : 1 , \"number_of_shards\" : 1 } } # Crea te some da ta i nt o t he i n dex PUT sample/_doc/ 1 { \"firstname\" : \"gaurav\" , \"lastname\" : \"sood\" } # Read t he da ta fr om t he i n dex GET sample/_doc/ 1 # Upda te t he da ta i n t he i n dex POST sample/_upda te / 1 { \"doc\" : { \"middlename\" : \"kumar\" } } # or usi n g scrip te d pai nless POST sample/_upda te / 1 { \"script\" : { \"lang\" : \"painless\" , \"source\" : \"ctx._source.remove('middlename')\" } } # DELETE doc a n d i n dex DELETE sample -1 /_doc/ 1 DELETE sample -1 From this understanding about APIs, - PUT operation is used to create a new objects - POST operation is used to update the objects","title":"CRUD Operations on a ES Index"},{"location":"indexing-data/Indexing-Data/","text":"Things covered here Define an Index That Satisfies a Given Set of Requirements Perform Index, Create, Read, Update, and Delete Operations on the Documents of an Index Define and Use Index Aliases Define and Use an Index Template for a Given Pattern that Satisfies a Given Set of Requirements Define and Use a Dynamic Template That Satisfies a Given Set of Requirements Use the Reindex API and Update by Query API to Reindex and/or Update Documents --> create a secondary cluster Define and Use an Ingest Pipeline That Satisfies a Given Set of Requirements, Including the Use of Painless to Modify Documents Indexing documents in bulk A good place to start is with batches of 1,000 to 5,000 documents and a total payload between 5MB and 15MB. From there, you can experiment to find the sweet spot.","title":"Indexing Data"},{"location":"indexing-data/Indexing-Data/#indexing-documents-in-bulk","text":"A good place to start is with batches of 1,000 to 5,000 documents and a total payload between 5MB and 15MB. From there, you can experiment to find the sweet spot.","title":"Indexing documents in bulk"},{"location":"indexing-data/Ingest-Pipelines/","text":"PUT _i n ges t /pipeli ne / test - pipeli ne { \"description\" : \"Converts M/F to Male/Female\" , \"processors\" : [ { \"remove\" : { \"field\" : \"account_number\" } }, { \"set\" : { \"field\" : \"_source.fullname\" , \"value\" : \"{{_source.firstname}} {{_source.lastname}}\" } }, { \"convert\" : { \"field\" : \"age\" , \"type\" : \"string\" } }, { \"script\" : { \"lang\" : \"painless\" , \"source\" : \"\"\" if(ctx.gender == \" M \") { ctx.gender = \" male \" } else { ctx.gender = \" fe male \" } \"\"\" } } ] } POST _rei n dex { \"source\" : { \"index\" : \"accounts\" }, \"dest\" : { \"pipeline\" : \"test-pipeline\" , // pipeli ne na me \"index\" : \"banking-new\" } } GET ba n ki n g - ne w/_search","title":"Ingest Pipelines"},{"location":"indexing-data/Loading-data/","text":"Loading Dummy Data Dummy data can be fetched from the below locations Accounts - https://raw.githubusercontent.com/linuxacademy/content-elastic-certification/master/sample_data/accounts.json Logs - https://raw.githubusercontent.com/linuxacademy/content-elastic-certification/master/sample_data/logs.json shakespeare - https://raw.githubusercontent.com/linuxacademy/content-elastic-certification/master/sample_data/shakespeare.json Sample commands to load the data, using curl curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/accounts/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @accounts.json >files.log curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/logs/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @logs.json >files.log curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/shakespeare/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @shakespeare.json >files.log for f in xa* do curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/logs/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @$f done splitting a file split -10000 shakespeare.json shake","title":"Loading data"},{"location":"indexing-data/Loading-data/#loading-dummy-data","text":"Dummy data can be fetched from the below locations Accounts - https://raw.githubusercontent.com/linuxacademy/content-elastic-certification/master/sample_data/accounts.json Logs - https://raw.githubusercontent.com/linuxacademy/content-elastic-certification/master/sample_data/logs.json shakespeare - https://raw.githubusercontent.com/linuxacademy/content-elastic-certification/master/sample_data/shakespeare.json Sample commands to load the data, using curl curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/accounts/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @accounts.json >files.log curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/logs/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @logs.json >files.log curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/shakespeare/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @shakespeare.json >files.log for f in xa* do curl -k -u elastic:RMa8k7nwIzBZZai3jRay https://localhost:9200/logs/_bulk\\?pretty -H \"Content-Type: application/x-ndjson\" --data-binary @$f done splitting a file split -10000 shakespeare.json shake","title":"Loading Dummy Data"},{"location":"indexing-data/Reindex-UpdateQuery/","text":"Reindex POST _rei n dex { \"source\" : { \"index\" : \"bank\" , \"query\" : { \"term\" : { \"gender.keyword\" : { \"value\" : \"M\" } } } }, \"dest\" : { \"index\" : \"bank-male\" } } update by query GET ba n k/_search POST ba n k/_upda te _by_query { \"script\" : { \"lang\" : \"painless\" , \"source\" : \"\"\" ctx._source.balance += ctx._source.balance*0.03; if (ctx._source.transactions == null) { ctx._source.transactions = 1; } else { ctx._source.transactions++; } \"\"\" }, \"query\" : { \"term\" : { \"gender.keyword\" : \"F\" } } } GET ba n k/_doc/ 13","title":"Reindex"},{"location":"indexing-data/Reindex-UpdateQuery/#reindex","text":"POST _rei n dex { \"source\" : { \"index\" : \"bank\" , \"query\" : { \"term\" : { \"gender.keyword\" : { \"value\" : \"M\" } } } }, \"dest\" : { \"index\" : \"bank-male\" } }","title":"Reindex"},{"location":"indexing-data/Reindex-UpdateQuery/#update-by-query","text":"GET ba n k/_search POST ba n k/_upda te _by_query { \"script\" : { \"lang\" : \"painless\" , \"source\" : \"\"\" ctx._source.balance += ctx._source.balance*0.03; if (ctx._source.transactions == null) { ctx._source.transactions = 1; } else { ctx._source.transactions++; } \"\"\" }, \"query\" : { \"term\" : { \"gender.keyword\" : \"F\" } } } GET ba n k/_doc/ 13","title":"update by query"},{"location":"indexing-data/Templates/","text":"ES Templates PUT _ te mpla te /sample { \"aliases\" : { \"test\" : {} }, \"mappings\" : { \"properties\" : { \"firstname\" : { \"type\" : \"keyword\" }, \"lastname\" : { \"type\" : \"keyword\" } } }, \"settings\" : { \"number_of_replicas\" : 1 , \"number_of_shards\" : 1 }, \"index_patterns\" : [ \"sample-*\" ], \"order\" : 0 } index_patterns - tells which indices will have this template order - defines which template to use in case of multiple matches Dynamic Templates PUT _ te mpla te /sample { \"aliases\" : { \"test\" : {} }, \"mappings\" : { \"properties\" : { \"firstname\" : { \"type\" : \"keyword\" } }, \"dynamic_templates\" : [ // Lis t o f dy na mic te mpla tes { \"strings_to_keyword\" : { // na me o f pa ttern \"match_mapping_type\" : \"string\" , // da ta f ield t o ma t ch \"unmatch\" : \"*_text\" , // u n ma t ch a f ield based o n key \"mapping\" : { \"type\" : \"keyword\" // mappi n g f or t he f ield } } }, { \"long_to_integer\" : { \"match_mapping_type\" : \"long\" , \"mapping\" : { \"type\" : \"integer\" } } }, { \"strings_to_text\" : { \"match_mapping_type\" : \"string\" , \"match\" : \"*_text\" , \"mapping\" : { \"type\" : \"text\" } } } ] }, \"settings\" : { \"number_of_replicas\" : 1 , \"number_of_shards\" : 1 }, \"index_patterns\" : [ \"sample-*\" ] }","title":"Templates"},{"location":"indexing-data/Templates/#es-templates","text":"PUT _ te mpla te /sample { \"aliases\" : { \"test\" : {} }, \"mappings\" : { \"properties\" : { \"firstname\" : { \"type\" : \"keyword\" }, \"lastname\" : { \"type\" : \"keyword\" } } }, \"settings\" : { \"number_of_replicas\" : 1 , \"number_of_shards\" : 1 }, \"index_patterns\" : [ \"sample-*\" ], \"order\" : 0 } index_patterns - tells which indices will have this template order - defines which template to use in case of multiple matches","title":"ES Templates"},{"location":"indexing-data/Templates/#dynamic-templates","text":"PUT _ te mpla te /sample { \"aliases\" : { \"test\" : {} }, \"mappings\" : { \"properties\" : { \"firstname\" : { \"type\" : \"keyword\" } }, \"dynamic_templates\" : [ // Lis t o f dy na mic te mpla tes { \"strings_to_keyword\" : { // na me o f pa ttern \"match_mapping_type\" : \"string\" , // da ta f ield t o ma t ch \"unmatch\" : \"*_text\" , // u n ma t ch a f ield based o n key \"mapping\" : { \"type\" : \"keyword\" // mappi n g f or t he f ield } } }, { \"long_to_integer\" : { \"match_mapping_type\" : \"long\" , \"mapping\" : { \"type\" : \"integer\" } } }, { \"strings_to_text\" : { \"match_mapping_type\" : \"string\" , \"match\" : \"*_text\" , \"mapping\" : { \"type\" : \"text\" } } } ] }, \"settings\" : { \"number_of_replicas\" : 1 , \"number_of_shards\" : 1 }, \"index_patterns\" : [ \"sample-*\" ] }","title":"Dynamic Templates"},{"location":"install-and-config/Install-and-Config/","text":"Steps to cover here Deploy, Configure, and Start an Elasticsearch Cluster That Satisfies a Given Set of Requirements Secure a Cluster Using Elasticsearch Security Define Role-Based Access Control Using Elasticsearch Security Deploy and Configure a Multi-Node Elasticsearch Cluster Encrypt Cluster and Client Elasticsearch Networks Configure User Access Control for Elasticsearch","title":"Install and Config"}]}