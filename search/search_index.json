{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ELK Stack Getting Started with ELK Stack.","title":"Welcome"},{"location":"#welcome-to-elk-stack","text":"Getting Started with ELK Stack.","title":"Welcome to ELK Stack"},{"location":"debugging/","text":"","title":"Debugging"},{"location":"Installation/install/","text":"On CentOS 7 Machine, elastic version 6.2.4 as this installs xpack, in newer versions xpack is installed by default Install java yum install java-1.8.0-openjdk -y Going via tar Start with adding a user elastic, sudo useradd elastic Increase security limits for the elastic user In /etc/security/limits.conf add elastic - nofile 65536 Increase Memory Map limits In /etc/sysctl.conf add vm.max_map_count = 262144 do sysctl -p to load system settings Download elasticsearch curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gz tar -xzvf test - start elastic and check if its working ./bin/elasticsearch -d -p pid less logs/elasticsearch kill `cat pid` Download Kibana curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gz Install x-pack curl -L -O https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.2.4.zip For Offline installation ./elasticsearch/bin/elasticsearch-plugin install file:///home/elastic/x-pack-6.2.4.zip Securing and Encryption In master node ~/elasticsearch/config/certs ~/elasticsearch/bin/x-pack/certutil ca passowod=a ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name master --dns gsood1c.mylabserver.com --ip 172.31.22.182 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data1 --dns gsood2c.mylabserver.com --ip 172.31.31.49 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data2 --dns gsood3c.mylabserver.com --ip 172.31.22.21 In other nodes copy scp data1.p12 elastic@172.31.31.49:/home/elastic/ ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: full xpack.security.transport.ssl.keystore.path: certs/master.p12 xpack.security.transport.ssl.truststore.path: certs/master.p12 xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: certs/master.p12 xpack.security.http.ssl.truststore.path: certs/master.p12 ## same in other nodes with correct keystore In Kibana ~/kibana/bin/kibana-plugin install file:///home/elastic/x-pack-6.2.4.zip","title":"Install"},{"location":"Installation/install/#going-via-tar","text":"Start with adding a user elastic, sudo useradd elastic Increase security limits for the elastic user In /etc/security/limits.conf add elastic - nofile 65536 Increase Memory Map limits In /etc/sysctl.conf add vm.max_map_count = 262144 do sysctl -p to load system settings Download elasticsearch curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gz tar -xzvf test - start elastic and check if its working ./bin/elasticsearch -d -p pid less logs/elasticsearch kill `cat pid` Download Kibana curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gz Install x-pack curl -L -O https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-6.2.4.zip For Offline installation ./elasticsearch/bin/elasticsearch-plugin install file:///home/elastic/x-pack-6.2.4.zip Securing and Encryption In master node ~/elasticsearch/config/certs ~/elasticsearch/bin/x-pack/certutil ca passowod=a ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name master --dns gsood1c.mylabserver.com --ip 172.31.22.182 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data1 --dns gsood2c.mylabserver.com --ip 172.31.31.49 ~/elasticsearch/bin/x-pack/certutil cert --ca elastic-stack-ca.p12 --name data2 --dns gsood3c.mylabserver.com --ip 172.31.22.21 In other nodes copy scp data1.p12 elastic@172.31.31.49:/home/elastic/ ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password ~/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: full xpack.security.transport.ssl.keystore.path: certs/master.p12 xpack.security.transport.ssl.truststore.path: certs/master.p12 xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: certs/master.p12 xpack.security.http.ssl.truststore.path: certs/master.p12 ## same in other nodes with correct keystore In Kibana ~/kibana/bin/kibana-plugin install file:///home/elastic/x-pack-6.2.4.zip","title":"Going via tar"},{"location":"Logstash/Readme/","text":"Configuring Logstash on an ELK Stack Pre Requisites Admin access to ELK Stack Logstash installed [refer to helm charts for chart] X-Pack enabled Current Configuration in Ops User - logstash-ingest Roles - Superadmin Steps followed Create the following roles to be used by logstash user - logstash-ingest logstash_writer, logstash_reader POST _xpack/security/role/logstash_writer { \"cluster\": [\"manage_index_templates\", \"monitor\"], \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"write\",\"delete\",\"create_index\"] } ] } POST _xpack/security/role/logstash_reader { \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"read\",\"view_index_metadata\"] } ] }","title":"Readme"},{"location":"Logstash/Readme/#configuring-logstash-on-an-elk-stack","text":"","title":"Configuring Logstash on an ELK Stack"},{"location":"Logstash/Readme/#pre-requisites","text":"Admin access to ELK Stack Logstash installed [refer to helm charts for chart] X-Pack enabled","title":"Pre Requisites"},{"location":"Logstash/Readme/#current-configuration-in-ops","text":"User - logstash-ingest Roles - Superadmin","title":"Current Configuration in Ops"},{"location":"Logstash/Readme/#steps-followed","text":"Create the following roles to be used by logstash user - logstash-ingest logstash_writer, logstash_reader POST _xpack/security/role/logstash_writer { \"cluster\": [\"manage_index_templates\", \"monitor\"], \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"write\",\"delete\",\"create_index\"] } ] } POST _xpack/security/role/logstash_reader { \"indices\": [ { \"names\": [ \"*\" ], \"privileges\": [\"read\",\"view_index_metadata\"] } ] }","title":"Steps followed"},{"location":"elasticsearch/Basic-Concepts/","text":"ELK Basic Concepts Near Real Time Elasticsearch is a near-realtime search platform. What this means is there is a slight latency (normally one second) from the time you index a document until the time it becomes searchable. Cluster A cluster is a collection of one or more nodes (servers) that together holds your entire data and provides federated indexing and search capabilities across all nodes. A cluster is identified by a unique name which by default is \"elasticsearch\". This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name. Make sure that you don\u2019t reuse the same cluster names in different environments, otherwise you might end up with nodes joining the wrong cluster. For instance you could use logging-dev, logging-stage, and logging-prod for the development, staging, and production clusters. Node A node is a single server that is part of your cluster, stores your data, and participates in the cluster\u2019s indexing and search capabilities. Just like a cluster, a node is identified by a name which by default is a random Universally Unique IDentifier (UUID) that is assigned to the node at startup. You can define any node name you want if you do not want the default. This name is important for administration purposes where you want to identify which servers in your network correspond to which nodes in your Elasticsearch cluster. A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named elasticsearch which means that if you start up a number of nodes on your network and\u2014assuming they can discover each other\u2014they will all automatically form and join a single cluster named elasticsearch. Index An index is a collection of documents that have somewhat similar characteristics. An index is identified by a name (that must be all lowercase) and this name is used to refer to the index when performing indexing, search, update, and delete operations against the documents in it. Type - Depreceated A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, e.g. one type for users, another type for blog posts. Reason - fields that have the same name in different mapping types are backed by the same Lucene field internally. This can lead to frustration when, for example, you want deleted to be a date field in one type and a boolean field in another type in the same index. On top of that, storing different entities that have few or no fields in common in the same index leads to sparse data and interferes with Lucene\u2019s ability to compress documents efficiently. Alternatives- 1. index per document type This approach has two benefits: Data is more likely to be dense and so benefit from compression techniques used in Lucene. The term statistics used for scoring in full text search are more likely to be accurate because all documents in the same index represent a single entity. custom type PUT twitter { \"mappings\": { \"_doc\": { \"properties\": { \"type\": { \"type\": \"keyword\" }, \"name\": { \"type\": \"text\" }, \"user_name\": { \"type\": \"keyword\" }, \"email\": { \"type\": \"keyword\" }, \"content\": { \"type\": \"text\" }, \"tweeted_at\": { \"type\": \"date\" } } } } } The explicit type field takes the place of the implicit _type field. Based on your search \"filter\": { \"match\": { \"type\": \"tweet\" or \"type\": \"user\" } } Refer - https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html Document - A document is a basic unit of information that can be indexed. This document is expressed in JSON (JavaScript Object Notation) which is a ubiquitous internet data interchange format. Shards and Replicas - Subdivide your index into multiple pieces called shards. When you create an index, you can simply define the number of shards that you want. Each shard is in itself a fully-functional and independent \"index\" that can be hosted on any node in the cluster. Sharding is important for two primary reasons: It allows you to horizontally split/scale your content volume It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput. Elasticsearch allows you to make one or more copies of your index\u2019s shards into what are called replica shards, or replicas for short. Replication is important for two primary reasons: It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from. It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel. The number of shards and replicas can be defined per index at the time the index is created. After the index is created, you may also change the number of replicas dynamically anytime. You can change the number of shards for an existing index using the _shrink and _split APIs, however this is not a trivial task and pre-planning for the correct number of shards is the optimal approach. Each Elasticsearch shard is a Lucene index. There is a maximum number of documents you can have in a single Lucene index. As of LUCENE-5843, the limit is 2,147,483,519 (= Integer.MAX_VALUE - 128) documents. You can monitor shard sizes using the _cat/shards API.","title":"Basic"},{"location":"elasticsearch/Basic-Concepts/#elk-basic-concepts","text":"","title":"ELK Basic Concepts"},{"location":"elasticsearch/Basic-Concepts/#near-real-time","text":"Elasticsearch is a near-realtime search platform. What this means is there is a slight latency (normally one second) from the time you index a document until the time it becomes searchable.","title":"Near Real Time"},{"location":"elasticsearch/Basic-Concepts/#cluster","text":"A cluster is a collection of one or more nodes (servers) that together holds your entire data and provides federated indexing and search capabilities across all nodes. A cluster is identified by a unique name which by default is \"elasticsearch\". This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name. Make sure that you don\u2019t reuse the same cluster names in different environments, otherwise you might end up with nodes joining the wrong cluster. For instance you could use logging-dev, logging-stage, and logging-prod for the development, staging, and production clusters.","title":"Cluster"},{"location":"elasticsearch/Basic-Concepts/#node","text":"A node is a single server that is part of your cluster, stores your data, and participates in the cluster\u2019s indexing and search capabilities. Just like a cluster, a node is identified by a name which by default is a random Universally Unique IDentifier (UUID) that is assigned to the node at startup. You can define any node name you want if you do not want the default. This name is important for administration purposes where you want to identify which servers in your network correspond to which nodes in your Elasticsearch cluster. A node can be configured to join a specific cluster by the cluster name. By default, each node is set up to join a cluster named elasticsearch which means that if you start up a number of nodes on your network and\u2014assuming they can discover each other\u2014they will all automatically form and join a single cluster named elasticsearch.","title":"Node"},{"location":"elasticsearch/Basic-Concepts/#index","text":"An index is a collection of documents that have somewhat similar characteristics. An index is identified by a name (that must be all lowercase) and this name is used to refer to the index when performing indexing, search, update, and delete operations against the documents in it.","title":"Index"},{"location":"elasticsearch/Basic-Concepts/#type-depreceated","text":"A type used to be a logical category/partition of your index to allow you to store different types of documents in the same index, e.g. one type for users, another type for blog posts. Reason - fields that have the same name in different mapping types are backed by the same Lucene field internally. This can lead to frustration when, for example, you want deleted to be a date field in one type and a boolean field in another type in the same index. On top of that, storing different entities that have few or no fields in common in the same index leads to sparse data and interferes with Lucene\u2019s ability to compress documents efficiently. Alternatives- 1. index per document type This approach has two benefits: Data is more likely to be dense and so benefit from compression techniques used in Lucene. The term statistics used for scoring in full text search are more likely to be accurate because all documents in the same index represent a single entity. custom type PUT twitter { \"mappings\": { \"_doc\": { \"properties\": { \"type\": { \"type\": \"keyword\" }, \"name\": { \"type\": \"text\" }, \"user_name\": { \"type\": \"keyword\" }, \"email\": { \"type\": \"keyword\" }, \"content\": { \"type\": \"text\" }, \"tweeted_at\": { \"type\": \"date\" } } } } } The explicit type field takes the place of the implicit _type field. Based on your search \"filter\": { \"match\": { \"type\": \"tweet\" or \"type\": \"user\" } } Refer - https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html","title":"Type - Depreceated"},{"location":"elasticsearch/Basic-Concepts/#document-","text":"A document is a basic unit of information that can be indexed. This document is expressed in JSON (JavaScript Object Notation) which is a ubiquitous internet data interchange format.","title":"Document -"},{"location":"elasticsearch/Basic-Concepts/#shards-and-replicas-","text":"Subdivide your index into multiple pieces called shards. When you create an index, you can simply define the number of shards that you want. Each shard is in itself a fully-functional and independent \"index\" that can be hosted on any node in the cluster. Sharding is important for two primary reasons: It allows you to horizontally split/scale your content volume It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput. Elasticsearch allows you to make one or more copies of your index\u2019s shards into what are called replica shards, or replicas for short. Replication is important for two primary reasons: It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from. It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel. The number of shards and replicas can be defined per index at the time the index is created. After the index is created, you may also change the number of replicas dynamically anytime. You can change the number of shards for an existing index using the _shrink and _split APIs, however this is not a trivial task and pre-planning for the correct number of shards is the optimal approach. Each Elasticsearch shard is a Lucene index. There is a maximum number of documents you can have in a single Lucene index. As of LUCENE-5843, the limit is 2,147,483,519 (= Integer.MAX_VALUE - 128) documents. You can monitor shard sizes using the _cat/shards API.","title":"Shards and Replicas -"},{"location":"elasticsearch/Exploring-Cluster/","text":"Cluster health GET /_cat/health?v pattern of how we access data in Elasticsearch. That pattern can be summarized as follows: <HTTP Verb> /<Index>/<Type>/<ID> cluster health, we either get green, yellow, or red. Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) Note: When a cluster is red, it will continue to serve search requests from the available shards but you will likely need to fix it ASAP since there are unassigned shards. Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html Cluster state updates The master node is the only node in a cluster that can make changes to the cluster state. The master node processes one cluster state update at a time, applies the required changes and publishes the updated cluster state to all the other nodes in the cluster. Each node receives the publish message, acknowledges it, but does not yet apply it. If the master does not receive acknowledgement from at least discovery.zen.minimum_master_nodes nodes within a certain time (controlled by the discovery.zen.commit_timeout setting and defaults to 30 seconds) the cluster state change is rejected. Once enough nodes have responded, the cluster state is committed and a message will be sent to all the nodes. The nodes then proceed to apply the new cluster state to their internal state. The master node waits for all nodes to respond, up to a timeout, before going ahead processing the next updates in the queue. The discovery.zen.publish_timeout is set by default to 30 seconds and is measured from the moment the publishing started. Both timeout settings can be changed dynamically through the cluster update settings api. https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html","title":"Exloring Cluster"},{"location":"elasticsearch/Exploring-Cluster/#cluster-health","text":"GET /_cat/health?v pattern of how we access data in Elasticsearch. That pattern can be summarized as follows: <HTTP Verb> /<Index>/<Type>/<ID> cluster health, we either get green, yellow, or red. Green - everything is good (cluster is fully functional) Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional) Red - some data is not available for whatever reason (cluster is partially functional) Note: When a cluster is red, it will continue to serve search requests from the available shards but you will likely need to fix it ASAP since there are unassigned shards. Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html Cluster state updates The master node is the only node in a cluster that can make changes to the cluster state. The master node processes one cluster state update at a time, applies the required changes and publishes the updated cluster state to all the other nodes in the cluster. Each node receives the publish message, acknowledges it, but does not yet apply it. If the master does not receive acknowledgement from at least discovery.zen.minimum_master_nodes nodes within a certain time (controlled by the discovery.zen.commit_timeout setting and defaults to 30 seconds) the cluster state change is rejected. Once enough nodes have responded, the cluster state is committed and a message will be sent to all the nodes. The nodes then proceed to apply the new cluster state to their internal state. The master node waits for all nodes to respond, up to a timeout, before going ahead processing the next updates in the queue. The discovery.zen.publish_timeout is set by default to 30 seconds and is measured from the moment the publishing started. Both timeout settings can be changed dynamically through the cluster update settings api. https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html","title":"Cluster health"},{"location":"elasticsearch/Modifying-Data/","text":"Updating Documnents Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. POST /customer/_doc/1/_update?pretty { \"doc\": { \"name\": \"Jane Doe\" } } Batch Processing In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as few network roundtrips as possible. The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.","title":"Modifying Data"},{"location":"elasticsearch/Modifying-Data/#updating-documnents","text":"Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. POST /customer/_doc/1/_update?pretty { \"doc\": { \"name\": \"Jane Doe\" } }","title":"Updating Documnents"},{"location":"elasticsearch/Modifying-Data/#batch-processing","text":"In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as few network roundtrips as possible. The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.","title":"Batch Processing"}]}